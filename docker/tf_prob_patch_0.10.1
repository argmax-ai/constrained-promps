diff --git a/tensorflow_probability/python/optimizer/lbfgs.py b/tensorflow_probability/python/optimizer/lbfgs.py
index 77a17bae3..719e26eaa 100644
--- a/tensorflow_probability/python/optimizer/lbfgs.py
+++ b/tensorflow_probability/python/optimizer/lbfgs.py
@@ -36,6 +36,7 @@ from tensorflow_probability.python.internal import distribution_util
 from tensorflow_probability.python.internal import prefer_static
 from tensorflow_probability.python.optimizer import bfgs_utils
 
+from tqdm import tqdm
 
 LBfgsOptimizerResults = collections.namedtuple(
     'LBfgsOptimizerResults', [
@@ -87,6 +88,7 @@ def minimize(value_and_gradients_function,
              max_iterations=50,
              parallel_iterations=1,
              stopping_condition=None,
+             one_step_callback=None,
              name=None):
   """Applies the L-BFGS algorithm to minimize a differentiable function.
 
@@ -217,6 +219,8 @@ def minimize(value_and_gradients_function,
         x_tolerance, dtype=dtype, name='x_tolerance')
     max_iterations = tf.convert_to_tensor(max_iterations, name='max_iterations')
 
+    if not (one_step_callback is None):
+      pbar = tqdm(total=max_iterations.numpy(), desc="L-BFGS")
     # The `state` here is a `LBfgsOptimizerResults` tuple with values for the
     # current state of the algorithm computation.
     def _cond(state):
@@ -247,17 +251,23 @@ def minimize(value_and_gradients_function,
           gradient_deltas=_queue_push(
               current_state.gradient_deltas, should_update,
               next_state.objective_gradient - current_state.objective_gradient))
+      if not (one_step_callback is None):
+        state_after_inv_hessian_update = one_step_callback(state_after_inv_hessian_update, pbar)
+        pbar.update(n=1)  # may trigger a refresh
       return [state_after_inv_hessian_update]
 
     initial_state = _get_initial_state(value_and_gradients_function,
                                        initial_position,
                                        num_correction_pairs,
                                        tolerance)
-    return tf.while_loop(
+    res = tf.while_loop(
         cond=_cond,
         body=_body,
         loop_vars=[initial_state],
         parallel_iterations=parallel_iterations)[0]
+    if not (one_step_callback is None):
+      pbar.close()
+    return res
 
 
 def _get_initial_state(value_and_gradients_function,
